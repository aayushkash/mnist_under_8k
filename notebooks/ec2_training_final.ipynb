{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "0m2JWFliFfKT"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn as nn\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "from torchsummary import summary\n",
    "from torch.utils.data import DataLoader, Subset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "DyVP1pKAlXpO"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_data_loaders(batch_size=64):\n",
    "\n",
    "    train_transform=transforms.Compose([\n",
    "                        transforms.Resize((28, 28)),\n",
    "                        transforms.RandomRotation((-7.0, 7.0), fill=(1,)),\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize((0.1307,), (0.3081,))\n",
    "                    ])\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "\n",
    "    # Load full datasets\n",
    "    train_dataset = datasets.MNIST('./data', train=True, download=True, transform=train_transform)\n",
    "    test_dataset = datasets.MNIST('./data', train=False, transform=transform)\n",
    "\n",
    "    # Create indices for 25% of training data\n",
    "    # total_train = len(train_dataset)\n",
    "    # indices = np.random.permutation(total_train)\n",
    "    # train_size = int(0.25 * total_train)  # 25% of the data\n",
    "    # train_indices = indices[:train_size]\n",
    "\n",
    "    # Create subset of training data\n",
    "    # train_dataset = Subset(train_dataset, train_indices)\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    # print(f\"Training with {train_size:,} samples (25% of original {total_train:,} samples)\")\n",
    "    print(f\"Training with {len(train_dataset)} samples\")\n",
    "\n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "1gyPRQm4e2Ai"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "DROP_OUT = 0.1\n",
    "\n",
    "class NetGAP(nn.Module):\n",
    "    \"\"\"\n",
    "    CNN with Global Average Pooling (GAP)\n",
    "==================================================\n",
    "Model Architecture: With GAP\n",
    "==================================================\n",
    "----------------------------------------------------------------\n",
    "        Layer (type)               Output Shape         Param #\n",
    "================================================================\n",
    "            Conv2d-1            [-1, 8, 28, 28]              80\n",
    "              ReLU-2            [-1, 8, 28, 28]               0\n",
    "       BatchNorm2d-3            [-1, 8, 28, 28]              16\n",
    "            Conv2d-4            [-1, 8, 28, 28]             584\n",
    "              ReLU-5            [-1, 8, 28, 28]               0\n",
    "       BatchNorm2d-6            [-1, 8, 28, 28]              16\n",
    "         MaxPool2d-7            [-1, 8, 14, 14]               0\n",
    "           Dropout-8            [-1, 8, 14, 14]               0\n",
    "            Conv2d-9           [-1, 16, 14, 14]           1,168\n",
    "             ReLU-10           [-1, 16, 14, 14]               0\n",
    "      BatchNorm2d-11           [-1, 16, 14, 14]              32\n",
    "           Conv2d-12           [-1, 16, 14, 14]           2,320\n",
    "             ReLU-13           [-1, 16, 14, 14]               0\n",
    "      BatchNorm2d-14           [-1, 16, 14, 14]              32\n",
    "        MaxPool2d-15             [-1, 16, 7, 7]               0\n",
    "          Dropout-16             [-1, 16, 7, 7]               0\n",
    "           Conv2d-17             [-1, 16, 7, 7]           2,320\n",
    "             ReLU-18             [-1, 16, 7, 7]               0\n",
    "      BatchNorm2d-19             [-1, 16, 7, 7]              32\n",
    "           Conv2d-20             [-1, 10, 7, 7]             170\n",
    "             ReLU-21             [-1, 10, 7, 7]               0\n",
    "      BatchNorm2d-22             [-1, 10, 7, 7]              20\n",
    "        MaxPool2d-23             [-1, 10, 3, 3]               0\n",
    "          Dropout-24             [-1, 10, 3, 3]               0\n",
    "AdaptiveAvgPool2d-25             [-1, 10, 1, 1]               0\n",
    "================================================================\n",
    "Total params: 6,790\n",
    "================================================================\n",
    "\n",
    "\n",
    "    Target:\n",
    "    - Replace FC layer with Global Average Pooling\n",
    "    - Reduce parameter count significantly under 8k\n",
    "    - Maintain or improve accuracy compared to FC models\n",
    "    - Make model morTraining With GAP model...\n",
    "Training with 60000 samples\n",
    "Epoch: 0 | Train Loss: 0.682 | Train Acc: 83.76% | Val Loss: 9.662 | Val Acc: 96.56% | Best Val Acc: 96.56%\n",
    "Epoch: 1 | Train Loss: 0.177 | Train Acc: 95.61% | Val Loss: 5.702 | Val Acc: 97.65% | Best Val Acc: 97.65%\n",
    "Epoch: 2 | Train Loss: 0.118 | Train Acc: 96.72% | Val Loss: 6.061 | Val Acc: 96.98% | Best Val Acc: 97.65%\n",
    "Epoch: 3 | Train Loss: 0.100 | Train Acc: 97.06% | Val Loss: 3.488 | Val Acc: 98.21% | Best Val Acc: 98.21%\n",
    "Epoch: 4 | Train Loss: 0.085 | Train Acc: 97.49% | Val Loss: 3.291 | Val Acc: 98.39% | Best Val Acc: 98.39%\n",
    "Epoch: 5 | Train Loss: 0.074 | Train Acc: 97.83% | Val Loss: 2.967 | Val Acc: 98.55% | Best Val Acc: 98.55%\n",
    "Epoch: 6 | Train Loss: 0.069 | Train Acc: 97.95% | Val Loss: 2.686 | Val Acc: 98.76% | Best Val Acc: 98.76%\n",
    "Epoch: 7 | Train Loss: 0.060 | Train Acc: 98.22% | Val Loss: 2.716 | Val Acc: 98.62% | Best Val Acc: 98.76%\n",
    "Epoch: 8 | Train Loss: 0.057 | Train Acc: 98.24% | Val Loss: 2.093 | Val Acc: 99.01% | Best Val Acc: 99.01%\n",
    "Epoch: 9 | Train Loss: 0.051 | Train Acc: 98.43% | Val Loss: 1.725 | Val Acc: 99.19% | Best Val Acc: 99.19%\n",
    "Epoch: 10 | Train Loss: 0.045 | Train Acc: 98.59% | Val Loss: 1.782 | Val Acc: 99.20% | Best Val Acc: 99.20%\n",
    "Epoch: 11 | Train Loss: 0.039 | Train Acc: 98.81% | Val Loss: 1.606 | Val Acc: 99.15% | Best Val Acc: 99.20%\n",
    "Epoch: 12 | Train Loss: 0.035 | Train Acc: 98.89% | Val Loss: 1.520 | Val Acc: 99.28% | Best Val Acc: 99.28%\n",
    "Epoch: 13 | Train Loss: 0.032 | Train Acc: 99.03% | Val Loss: 1.446 | Val Acc: 99.29% | Best Val Acc: 99.29%\n",
    "Epoch: 14 | Train Loss: 0.030 | Train Acc: 99.09% | Val Loss: 1.425 | Val Acc: 99.34% | Best Val Acc: 99.34%\n",
    "\n",
    "===================================================\n",
    "  Receptive Field (RF) calculation:\n",
    "===================================================\n",
    "\n",
    "    RF = 1 + sum((kernel_size - 1) * stride_product)\n",
    "    stride_product = product of all previous strides\n",
    "\n",
    "    Layer details:\n",
    "    Conv1: RF_in=1, k=3, s=1, p=1 → RF_out=3\n",
    "    Conv2: RF_in=3, k=3, s=1, p=1 → RF_out=5\n",
    "    MaxPool1: RF_in=5, k=2, s=2 → RF_out=6\n",
    "\n",
    "    Conv3: RF_in=6, k=3, s=1, p=1 → RF_out=10\n",
    "    Conv4: RF_in=10, k=3, s=1, p=1 → RF_out=14\n",
    "    MaxPool2: RF_in=14, k=2, s=2 → RF_out=16\n",
    "\n",
    "    Conv5: RF_in=16, k=3, s=1, p=1 → RF_out=20\n",
    "    Conv6: RF_in=20, k=1, s=1 → RF_out=20\n",
    "    MaxPool3: RF_in=20, k=2, s=2 → RF_out=22\n",
    "\n",
    "    Final RF = 22x22 pixels\n",
    "\n",
    "===================================================\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, dropout_rate=DROP_OUT):\n",
    "        super(NetGAP, self).__init__()\n",
    "\n",
    "        # First block: RF 1→3→5→6\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1,\n",
    "                     out_channels=8,\n",
    "                     kernel_size=3,\n",
    "                     padding=1,\n",
    "                     stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(num_features=8),\n",
    "            nn.Conv2d(in_channels=8,\n",
    "                     out_channels=8,\n",
    "                     kernel_size=3,\n",
    "                     padding=1,\n",
    "                     stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(num_features=8),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # /2\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "        )\n",
    "\n",
    "        # Second block: RF 6→10→14→16\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=8,\n",
    "                     out_channels=16,\n",
    "                     kernel_size=3,\n",
    "                     padding=1,\n",
    "                     stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(num_features=16),\n",
    "            nn.Conv2d(in_channels=16,\n",
    "                     out_channels=16,\n",
    "                     kernel_size=3,\n",
    "                     padding=1,\n",
    "                     stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(num_features=16),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # /2\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "        )\n",
    "\n",
    "        # Third block: RF 16→20→20→22\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=16,\n",
    "                     out_channels=16,\n",
    "                     kernel_size=3,\n",
    "                     padding=1,\n",
    "                     stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(num_features=16),\n",
    "            nn.Conv2d(in_channels=16,\n",
    "                     out_channels=10,\n",
    "                     kernel_size=1,  # 1x1 conv for channel reduction\n",
    "                     stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(num_features=10),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # /2\n",
    "            nn.Dropout(p=dropout_rate),\n",
    "        )\n",
    "\n",
    "        # Global Average Pooling: maintains RF while reducing spatial dims to 1x1\n",
    "        self.gap = nn.AdaptiveAvgPool2d(output_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)  # 28x28 -> 14x14\n",
    "        x = self.conv2(x)  # 14x14 -> 7x7\n",
    "        x = self.conv3(x)  # 7x7 -> 3x3\n",
    "        x = self.gap(x)    # 3x3 -> 1x1\n",
    "        x = x.view(-1, 10)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ICIkipAgn5uF",
    "outputId": "a917b6fe-6cca-421b-f577-3be4acacfebf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1            [-1, 8, 28, 28]              80\n",
      "              ReLU-2            [-1, 8, 28, 28]               0\n",
      "       BatchNorm2d-3            [-1, 8, 28, 28]              16\n",
      "            Conv2d-4            [-1, 8, 28, 28]             584\n",
      "              ReLU-5            [-1, 8, 28, 28]               0\n",
      "       BatchNorm2d-6            [-1, 8, 28, 28]              16\n",
      "         MaxPool2d-7            [-1, 8, 14, 14]               0\n",
      "           Dropout-8            [-1, 8, 14, 14]               0\n",
      "            Conv2d-9           [-1, 16, 14, 14]           1,168\n",
      "             ReLU-10           [-1, 16, 14, 14]               0\n",
      "      BatchNorm2d-11           [-1, 16, 14, 14]              32\n",
      "           Conv2d-12           [-1, 16, 14, 14]           2,320\n",
      "             ReLU-13           [-1, 16, 14, 14]               0\n",
      "      BatchNorm2d-14           [-1, 16, 14, 14]              32\n",
      "        MaxPool2d-15             [-1, 16, 7, 7]               0\n",
      "          Dropout-16             [-1, 16, 7, 7]               0\n",
      "           Conv2d-17             [-1, 16, 7, 7]           2,320\n",
      "             ReLU-18             [-1, 16, 7, 7]               0\n",
      "      BatchNorm2d-19             [-1, 16, 7, 7]              32\n",
      "           Conv2d-20             [-1, 10, 7, 7]             170\n",
      "             ReLU-21             [-1, 10, 7, 7]               0\n",
      "      BatchNorm2d-22             [-1, 10, 7, 7]              20\n",
      "        MaxPool2d-23             [-1, 10, 3, 3]               0\n",
      "          Dropout-24             [-1, 10, 3, 3]               0\n",
      "AdaptiveAvgPool2d-25             [-1, 10, 1, 1]               0\n",
      "================================================================\n",
      "Total params: 6,790\n",
      "Trainable params: 6,790\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.50\n",
      "Params size (MB): 0.03\n",
      "Estimated Total Size (MB): 0.53\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(device)\n",
    "model = NetGAP().to(device)\n",
    "summary(model, input_size=(1, 28, 28))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "esedu7nKlfhZ"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    \"\"\"Set seeds for reproducibility.\"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "def setup_logger():\n",
    "    log_dir = Path('logs')\n",
    "    log_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    logging.basicConfig(\n",
    "        filename=f'logs/training_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.log',\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(message)s'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "hqU9eSVZl3MN"
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_device():\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "        device_name = \"Apple Silicon (M1/M2)\"\n",
    "    elif torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        device_name = f\"CUDA ({torch.cuda.get_device_name(0)})\"\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        device_name = \"CPU\"\n",
    "    return device, device_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "xdydjYTZFyi3"
   },
   "outputs": [],
   "source": [
    "\n",
    "def train(epochs=15, batch_size=64, learning_rate=0.01, target_accuracy=99.4):\n",
    "    set_seed(42)  # Set seed for reproducibility\n",
    "    setup_logger()\n",
    "\n",
    "    # Device setup\n",
    "    device, device_name = get_device()\n",
    "    gpu_info = f\"Using device: {device} ({device_name})\"\n",
    "\n",
    "    if device.type == 'cuda':\n",
    "        gpu_info += f\"\\nMemory Usage:\"\n",
    "        gpu_info += f\"\\n  Allocated: {round(torch.cuda.memory_allocated(0)/1024**2,1)} MB\"\n",
    "        gpu_info += f\"\\n  Cached:    {round(torch.cuda.memory_reserved(0)/1024**2,1)} MB\"\n",
    "\n",
    "    print(gpu_info)\n",
    "    logging.info(gpu_info)\n",
    "\n",
    "    models = [\n",
    "        (\"With GAP\", NetGAP())\n",
    "    ]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for name, model in models:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Model Architecture: {name}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        # summary(model, input_size=(1, 28, 28))\n",
    "\n",
    "        model = model.to(device)\n",
    "        # Print model summary before training\n",
    "        print(f\"\\nTraining {name} model...\")\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "        train_loader, test_loader = get_data_loaders(batch_size)\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer,max_lr=0.01,epochs=epochs,\n",
    "                                                        steps_per_epoch=len(train_loader))\n",
    "\n",
    "\n",
    "        best_accuracy = 0.0\n",
    "        early_stop = False\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            if early_stop:\n",
    "                break\n",
    "\n",
    "            model.train()\n",
    "            train_loss = 0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "\n",
    "            for batch_idx, (data, target) in enumerate(train_loader):\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                output = model(data)\n",
    "                loss = F.nll_loss(output, target)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "\n",
    "                train_loss += loss.item()\n",
    "                _, predicted = output.max(1)\n",
    "                total += target.size(0)\n",
    "                correct += predicted.eq(target).sum().item()\n",
    "\n",
    "            train_accuracy = 100. * correct / total\n",
    "            train_loss = train_loss / len(train_loader)\n",
    "\n",
    "            # Validation\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for data, target in test_loader:\n",
    "                    data, target = data.to(device), target.to(device)\n",
    "                    output = model(data)\n",
    "                    val_loss += F.nll_loss(output, target, reduction='sum').item()\n",
    "                    _, predicted = output.max(1)\n",
    "                    total += target.size(0)\n",
    "                    correct += predicted.eq(target).sum().item()\n",
    "\n",
    "            val_accuracy = 100. * correct / total\n",
    "            val_loss = val_loss / len(test_loader)\n",
    "\n",
    "            # Update best accuracy\n",
    "            if val_accuracy > best_accuracy:\n",
    "                best_accuracy = val_accuracy\n",
    "\n",
    "            # Check for early stopping\n",
    "            if val_accuracy >= target_accuracy:\n",
    "                # early_stop = True\n",
    "                print(f\"\\nReached target accuracy of {target_accuracy}% at epoch {epoch}\")\n",
    "                logging.info(f\"Reached target accuracy of {target_accuracy}% at epoch {epoch}\")\n",
    "\n",
    "            # Log epoch results\n",
    "            log_message = (f'Epoch: {epoch} | '\n",
    "                        f'Train Loss: {train_loss:.3f} | '\n",
    "                        f'Train Acc: {train_accuracy:.2f}% | '\n",
    "                        f'Val Loss: {val_loss:.3f} | '\n",
    "                        f'Val Acc: {val_accuracy:.2f}% | '\n",
    "                        f'Best Val Acc: {best_accuracy:.2f}%')\n",
    "            logging.info(log_message)\n",
    "            print(log_message)\n",
    "\n",
    "        results.append((name, best_accuracy))\n",
    "        # Log GPU memory only for CUDA devices\n",
    "        if device.type == 'cuda':\n",
    "            memory_info = (f\"GPU Memory: \"\n",
    "                        f\"Allocated: {round(torch.cuda.memory_allocated(0)/1024**2,1)} MB, \"\n",
    "                        f\"Cached: {round(torch.cuda.memory_reserved(0)/1024**2,1)} MB\")\n",
    "            logging.info(memory_info)\n",
    "            print(memory_info)\n",
    "\n",
    "    print(\"\\nFinal Results:\")\n",
    "    for name, acc in results:\n",
    "        print(f\"{name}: {acc:.2f}%\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N8_u-C27l9wQ",
    "outputId": "ba264ba5-47f8-48e7-a737-f6c8f598be7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda (CUDA (NVIDIA L4))\n",
      "Memory Usage:\n",
      "  Allocated: 0.0 MB\n",
      "  Cached:    22.0 MB\n",
      "\n",
      "==================================================\n",
      "Model Architecture: With GAP\n",
      "==================================================\n",
      "\n",
      "Training With GAP model...\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9.91M/9.91M [00:01<00:00, 5.38MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 28.9k/28.9k [00:00<00:00, 157kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1.65M/1.65M [00:02<00:00, 738kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 403: Forbidden\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4.54k/4.54k [00:00<00:00, 9.74MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Training with 60000 samples\n",
      "Epoch: 0 | Train Loss: 0.803 | Train Acc: 79.13% | Val Loss: 10.706 | Val Acc: 96.57% | Best Val Acc: 96.57%\n",
      "Epoch: 1 | Train Loss: 0.196 | Train Acc: 95.11% | Val Loss: 5.443 | Val Acc: 97.69% | Best Val Acc: 97.69%\n",
      "Epoch: 2 | Train Loss: 0.132 | Train Acc: 96.33% | Val Loss: 4.072 | Val Acc: 98.02% | Best Val Acc: 98.02%\n",
      "Epoch: 3 | Train Loss: 0.113 | Train Acc: 96.64% | Val Loss: 3.087 | Val Acc: 98.45% | Best Val Acc: 98.45%\n",
      "Epoch: 4 | Train Loss: 0.093 | Train Acc: 97.26% | Val Loss: 3.231 | Val Acc: 98.37% | Best Val Acc: 98.45%\n",
      "Epoch: 5 | Train Loss: 0.085 | Train Acc: 97.45% | Val Loss: 2.337 | Val Acc: 98.83% | Best Val Acc: 98.83%\n",
      "Epoch: 6 | Train Loss: 0.076 | Train Acc: 97.83% | Val Loss: 1.886 | Val Acc: 99.11% | Best Val Acc: 99.11%\n",
      "Epoch: 7 | Train Loss: 0.069 | Train Acc: 98.00% | Val Loss: 1.583 | Val Acc: 99.23% | Best Val Acc: 99.23%\n",
      "Epoch: 8 | Train Loss: 0.061 | Train Acc: 98.14% | Val Loss: 1.594 | Val Acc: 99.20% | Best Val Acc: 99.23%\n",
      "Epoch: 9 | Train Loss: 0.057 | Train Acc: 98.28% | Val Loss: 1.556 | Val Acc: 99.29% | Best Val Acc: 99.29%\n",
      "Epoch: 10 | Train Loss: 0.050 | Train Acc: 98.47% | Val Loss: 1.645 | Val Acc: 99.18% | Best Val Acc: 99.29%\n",
      "Epoch: 11 | Train Loss: 0.044 | Train Acc: 98.65% | Val Loss: 1.390 | Val Acc: 99.30% | Best Val Acc: 99.30%\n",
      "\n",
      "Reached target accuracy of 99.4% at epoch 12\n",
      "Epoch: 12 | Train Loss: 0.040 | Train Acc: 98.78% | Val Loss: 1.279 | Val Acc: 99.40% | Best Val Acc: 99.40%\n",
      "\n",
      "Reached target accuracy of 99.4% at epoch 13\n",
      "Epoch: 13 | Train Loss: 0.037 | Train Acc: 98.84% | Val Loss: 1.236 | Val Acc: 99.45% | Best Val Acc: 99.45%\n",
      "\n",
      "Reached target accuracy of 99.4% at epoch 14\n",
      "Epoch: 14 | Train Loss: 0.036 | Train Acc: 98.90% | Val Loss: 1.202 | Val Acc: 99.43% | Best Val Acc: 99.45%\n",
      "GPU Memory: Allocated: 0.2 MB, Cached: 28.0 MB\n",
      "\n",
      "Final Results:\n",
      "With GAP: 99.45%\n"
     ]
    }
   ],
   "source": [
    "final_results = train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mMOwpY1BmC08",
    "outputId": "2889b9d1-8fb4-4d05-e021-3b56e414beaa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('With GAP', 99.45)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "5M4K14KIuovC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ip-172-31-9-229\n"
     ]
    }
   ],
   "source": [
    "!hostname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
